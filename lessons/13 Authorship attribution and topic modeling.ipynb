{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship attribution of a text corpus\n",
    "=======================================\n",
    "\n",
    "Here we will repeat a famous experiment in authorship attribution, and try to discover who wrote the Federalist Papers!\n",
    "\n",
    "We have the corpus from last week's lesson with NLTK; this week, we are going to use a library called `gensim` which has support for a lot of the sorts of big-data distant-reading text analysis that happens where DH intersects literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data\n",
    "-------------------\n",
    "\n",
    "Now let's load up the Papers. They are in a folder called 'federalist' and each paper is numbered, e.g. 'federalist_7.txt'. We can just as easily do this using NLTK to make a corpus out of the folder, as we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus.reader.util import read_regexp_block\n",
    "\n",
    "# Define how paragraphs look in our text files.\n",
    "def read_hanging_block( stream ):\n",
    "    return read_regexp_block( stream, \"^[A-Za-z]\" )\n",
    "\n",
    "corpus_root = '../textfiles/federalist'\n",
    "file_pattern = 'federalist_.*\\.txt'\n",
    "federalist = PlaintextCorpusReader( corpus_root, file_pattern, \n",
    "                                para_block_reader=read_hanging_block )\n",
    "print(\"List of texts in corpus:\", federalist.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorship attribution is done by comparing different *features* of the texts we are looking at. Examples include:\n",
    "\n",
    "* lexical features (average sentence length, variation in sentence length, range of words used)\n",
    "* punctuation features (average number of different marks per sentence)\n",
    "* word count features (e.g. frequency of the different common 'function words')\n",
    "* syntactic features (e.g. frequency of noun use, verb use, adjective use, etc.)\n",
    "\n",
    "Essentially there are a whole lot of approaches to take, and usually you want to take as many approaches as possible to arrive at some sort of consensus answer. Today we will try three approaches: looking at use of function words, at lexical diversity, and at relative frequency of parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the word count feature - the frequency of \"function words\"\n",
    "------------------------------------------------------------------\n",
    "\n",
    "These are the words that we would normally leave out of any vocabulary analysis because they are so common - 'the', 'a', 'and', 'of', 'to', and so on. Lists of them can be had for different languages, and indeed we know that NLTK provides us with such a list. Let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Now we have, for each text, to count up the frequency of each of these words. This is called making a \"feature vector\" - each text will be reduced to a data structure that has a count for each of the function words.\n",
    "\n",
    "**PAY ATTENTION HERE!** This step, the conversion of text files to feature vectors, is where you will make or break any of these text analysis techniques. As we will see, when we are doing authorship attribution we want to count the stopwords, but when we do topic modeling we want to count everything *BUT* the stopwords! Think carefully about the theory and ideas behind what you are doing, when you use these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have our \"texts\", which are lists of stopwords, and we have our dictionary, which assigns a unique ID to each word. We put these things together to make a vector of each text, which will be a series of `(dictionaryID, count)` tuples. Anytime the count is zero, the dictionary ID will simply be left out of that text's vector. We will use the `doc2bow` method to do this; the result looks something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first text, and now we want this sort of \"bag of words\" (bow) for all of the texts! We use a list comprehension again to get that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something similar to get the distribution of parts of speech. We will POS-tag all the texts, choose the twenty most common parts of speech throughout the corpus excluding punctuation, and then make a similar vector for each text counting the instances of each part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just as before, make a dictionary out of these \"texts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, let's filter out the punctuation, and limit ourselves to the top 15 parts of speech. We can filter the dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! We have our dictionary the way we want it, so we can make a second gensim corpus out of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have made two corpora from our texts; one represents the frequency of function words, and the other represents the frequency of common parts of speech.\n",
    "\n",
    "But now we will want to normalize our vectors a little bit - some texts are a lot longer than others, so will have many more function words overall, and we don't want this fact to affect our results. So we need to scale the values in each tuple, keeping them in proportion with each other but always between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's do some math\n",
    "def scale(vector):\n",
    "    size = 0.0\n",
    "    maximum = 0.0\n",
    "    for t in vector:\n",
    "        size += t[1]\n",
    "        if t[1] > maximum:\n",
    "            maximum = t[1]\n",
    "    scaled = []\n",
    "    for t in vector:\n",
    "        fpcount = float(t[1])\n",
    "        factor = size / size * maximum\n",
    "        scaled.append((t[0], fpcount / factor))\n",
    "    return scaled\n",
    "\n",
    "# Now let's apply this to scale the vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the results\n",
    "-------------------\n",
    "Okay! We have a set of criteria - the frequency of our function words - and a corresponding set of values for each text. It's time to crunch the numbers and see which papers resemble each other.\n",
    "\n",
    "We know that there were three authors, so we want to see if we can make the 85 different papers cluster into three groups. There is a statistical function for this called KMeans, from the \"scikit-learn\" module which has a lot of things for machine learning. (Dividing data into clusters of similar things is a pretty common thing to have to do in machine learning. Lucky for us.)\n",
    "\n",
    "First we define a function to do the clustering for each data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def PredictAuthors(feature_vector_set):\n",
    "    km = KMeans(n_clusters=3)\n",
    "    km.fit(feature_vector_set)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this, we need to convert our gensim corpus into a matrix that SciPy recognizes. Gensim gives us a utility to do this. In order to get our matrix the right way around, we will also have to transpose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we run this on our data table of the function word frequencies and get a complicated result. We ask for the labels of that result and get something that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these numbers (0, 1, 2) represents an author. We know that Hamilton was responsible for most of the papers, Madison for most of the rest, and Jay for the fewest. So let's assign the authors on that assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this into a function definition, since we'll have to do it twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how did that do against reality? Let's read in the real answers and add them to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an HTML table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# prepare our data\n",
    "\n",
    "\n",
    "# make our table\n",
    "answer_table = '<table><tr><th>Paper</th><th>Stopwords</th><th>Parts of speech</th><th>Real</th></tr>'\n",
    "for i in range(len(real_author)):\n",
    "    ra = real_author[i]\n",
    "    sa = stopword_authors[i]\n",
    "    pa = pos_authors[i]\n",
    "    answer_table += '<tr><td>%d</td>' % (i+1)     # Print the letter number\n",
    "    answer_table += '<td style=\"color: %s;\">%s</td>' % (colorcode(sa, ra), sa)\n",
    "    answer_table += '<td style=\"color: %s;\">%s</td>' % (colorcode(pa, ra), pa)\n",
    "    answer_table += '<td>%s</td></tr>' % ra\n",
    "answer_table += '</table>'\n",
    "\n",
    "HTML(answer_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...As you can see, the method is not perfect. 😄 A better method for the Federalist Papers problem would be to use a *trained* corpus, to let the model take into account what we know about the papers' authorship. \n",
    "\n",
    "Probably the most commonly-used method for authorship attribution today is known as Burrows' Delta, named after John Burrows who came up with it. The Delta algorithms are available in [a statistical package](https://sites.google.com/site/computationalstylistics/) called `stylo`, written for the R programming language for statistical computing. If this is something you anticipate wanting to use, that is a very good place to start."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
